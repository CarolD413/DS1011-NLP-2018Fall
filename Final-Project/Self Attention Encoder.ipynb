{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fall2018 DS-GA 1011 NLP Final Project\n",
    "------\n",
    "Team: Weicheng Zhu,Yiyi Zhang, Zhengyuan Ding, Zihao Zhao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic\n",
    "----\n",
    "The goal of this project is for you to build a neural machine translation system and experience how recent advances have made their way. Each team will build the following sequence of neural translation systems for two language pairs, Vietnamese (Vi)→English (En) and Chinese (Zh)→En (prepared corpora will be provided):\n",
    "\n",
    "- Recurrent neural network based encoder-decoder without attention\n",
    "- Recurrent neural network based encoder-decoder with attention\n",
    "- Replace the recurrent encoder with either convolutional or self-attention based encoder.\n",
    "- [Optional] Build either or both fully self-attention translation system or/and multilingual translation system.\n",
    "\n",
    "You are expected to implement these on your own (if necessary), experiment them with both language pairs, report their performance (measured in terms of automatic evaluation metrics) and analyze their behaviours and properties. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import related packages\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sacrebleu import corpus_bleu\n",
    "from tensorboardX import SummaryWriter\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the token index for pad, sos, eos and unk in the dictionary\n",
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Fix the tag for pad, sos, eos and unk in the dictionary\n",
    "PAD_TAG = \"<pad>\"\n",
    "SOS_TAG = \"<sos>\"\n",
    "EOS_TAG = \"<eos>\"\n",
    "UNK_TAG = \"<unk>\"\n",
    "\n",
    "#fix the maximum sentence length to filter\n",
    "MAX_LEN = 200\n",
    "\n",
    "class Lang:\n",
    "    \"\"\"A language vocabulary\n",
    "    \n",
    "    Attributes:\n",
    "        name: the name of the language\n",
    "        word2index: a dict where keys are words and values are indices\n",
    "        word2count: a dict where keys are words and values are corresponding word count \n",
    "        index2word: a dict where keys are indices and values are words\n",
    "        n_words: number of words in the vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        \"\"\"Initialize the Lang object with given language name as input\"\"\"\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        #Set initial index2word with pad, sos, eos and unk\n",
    "        self.index2word = {0: PAD_TAG, 1: SOS_TAG,2:EOS_TAG, 3:UNK_TAG} \n",
    "        # Count PAD, SOS, EOS, UNK\n",
    "        self.n_words = 4\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        \"\"\" Tokenizes the input sentence and add the tokens to the language vocabulary\"\"\"\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        \"\"\" Adds the given word into the language vocabulary if not existed\"\"\"\n",
    "        #check if the word is in the vocabulary using word2index\n",
    "        if word not in self.word2index: \n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1 \n",
    "            self.index2word[self.n_words] = word \n",
    "            self.n_words += 1\n",
    "        # if the given word is already in the vocabulary\n",
    "        else: \n",
    "            self.word2count[word] += 1 \n",
    "            \n",
    "def unicodeToAscii(s):\n",
    "    \"\"\"\n",
    "    Turn a Unicode string to plain ASCII, thanks to \n",
    "    http://stackoverflow.com/a/518232/2809427\n",
    "    \"\"\"\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    \"\"\"Lowercase, trim, and remove non-letter characters\"\"\"\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def removeSpace(s):\n",
    "    \"\"\"Replace multiple spaces in Chinese tokens by just one space\"\"\"\n",
    "    s = re.sub(' +',' ', s).strip()\n",
    "    return s\n",
    "\n",
    "def readLangs(lang1, lang2, datasettype, reverse=False):\n",
    "    \"\"\"Read token files and store them as Lang objects\n",
    "    \n",
    "    Args:\n",
    "        lang1, lang2: two language names\n",
    "        datasettype: a string('train'/'dev'/'test') to indicate train/validatoin/test set\n",
    "        reverse: whether to reverse order of two languages\n",
    "        \n",
    "    Returns:\n",
    "        input_lang: Lang object of input language vocabulary\n",
    "        output_lang: Lang object of output language vocabulary\n",
    "        pairs: list of sentence pairs\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    filename1 = '%s.tok.%s' % (datasettype, lang1)\n",
    "    filename2 = '%s.tok.%s' % (datasettype, lang2)\n",
    "        \n",
    "    # Read the file and split into lines\n",
    "    lines_1 = open(filename1, encoding='utf-8').read().strip().split('\\n')\n",
    "    lines_2 = open(filename2, encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(lines_1[i]),removeSpace(lines_2[i])] for i in range(len(lines_1))]\n",
    "    print('Pair1:', pairs[1])\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    #if reverse is True, input language is lang2 and output language is lang1\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    \"\"\"Check whether the sentence pair exceed MAX_LEN\n",
    "    \n",
    "    Arg:\n",
    "        p: a list consists of one sentence pair\n",
    "        \n",
    "    Returns:\n",
    "        a boolean to indicate whether to filter the given sentence pair\n",
    "    \"\"\"\n",
    "    return len(p[0].split(' ')) < MAX_LEN and len(p[1].split(' ')) < MAX_LEN \n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    \"\"\"Filter out sentence pairs that exceed MAX_LEN\n",
    "    \n",
    "    Arg: \n",
    "        pairs: list of sentence pairs\n",
    "    \n",
    "    Returns:\n",
    "        a list filtered sentence pairs\n",
    "    \"\"\"\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "def prepareData(lang1, lang2, datasettype, reverse=False):\n",
    "    \"\"\"Preprocess the token files\n",
    "    \n",
    "     Args:\n",
    "        lang1, lang2: two language names\n",
    "        datasettype: a string('train'/'dev'/'test') to indicate train/validatoin/test set\n",
    "        reverse: whether to reverse order of two languages\n",
    "        \n",
    "    Returns:\n",
    "        input_lang: Lang object of input language vocabulary after preprocessing\n",
    "        output_lang: Lang object of output language vocabulary after preprocessing\n",
    "        pairs: list of sentence pairs after preprocessing\n",
    "    \"\"\"\n",
    "    #read token files and store in Lang objects\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, datasettype, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    #filter sentence pairs\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    #build language vocabularies\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# input_lang, output_lang, pairs = prepareData('en', 'zh', 'train', True)\n",
    "# print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Dataset Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Pair1: ['with vibrant video clips captured by submarines david gallo takes us to some of earth apos s darkest most violent toxic and beautiful habitats the valleys and volcanic ridges of the oceans apos depths where life is bizarre resilient and shockingly abundant .', '大卫 盖罗 通过 潜水 潜水艇 拍下 的 影片 把 我们 带到 了 地球 最 黑暗 最 险恶 同时 也 最美 美丽 的 生物 栖息 栖息地 这里 是 海洋 深处 的 峡谷 和 火山 山脊 这里 怪诞 适应 适应力 应力 强 而且 数量 惊人 的 生命']\n",
      "Read 213376 sentence pairs\n",
      "Trimmed to 213334 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 88784\n",
      "en 50875\n",
      "Reading lines...\n",
      "Pair1: ['my father was listening to bbc news on his small gray radio .', '我 的 父亲 在 用 他 的 灰色 小 收音 收音机 听 BBC 新闻']\n",
      "Read 1261 sentence pairs\n",
      "Trimmed to 1261 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 6133\n",
      "en 3671\n",
      "Number of source-target pairs: 1261\n",
      "Input language: zh(6133)\n",
      "Output language: en(3671)\n",
      "Reading lines...\n",
      "Pair1: ['with vibrant video clips captured by submarines david gallo takes us to some of earth apos s darkest most violent toxic and beautiful habitats the valleys and volcanic ridges of the oceans apos depths where life is bizarre resilient and shockingly abundant .', '大卫 盖罗 通过 潜水 潜水艇 拍下 的 影片 把 我们 带到 了 地球 最 黑暗 最 险恶 同时 也 最美 美丽 的 生物 栖息 栖息地 这里 是 海洋 深处 的 峡谷 和 火山 山脊 这里 怪诞 适应 适应力 应力 强 而且 数量 惊人 的 生命']\n",
      "Read 213376 sentence pairs\n",
      "Trimmed to 213334 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 88784\n",
      "en 50875\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    \"\"\"Convert given sentence to vectors according to language vocabulary\"\"\"\n",
    "    #set word to unknown index if not in the vocabulary\n",
    "    return [lang.word2index[word]  \n",
    "            if word in lang.word2index else 3 for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    \"\"\"Convert given sentence to tensor\"\"\"\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    #append <eos> to each sentence to indicate the end\n",
    "    indexes.append(EOS_TOKEN)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device)\n",
    "    \n",
    "def tensorsFromPair(pair):\n",
    "    \"\"\"Convert sentence pairs to input tensors and output tensors\"\"\"\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1]) \n",
    "    return (input_tensor, target_tensor)\n",
    "    \n",
    "\n",
    "class Dataset(Dataset):\n",
    "    \"\"\"Construct dataset fit for pytorch input\n",
    "    \n",
    "    Attributes:\n",
    "        input_lang: a Lang object of input language vocabulary\n",
    "        output_lange: a Lang object of output language vocabulary\n",
    "        pairs: list of sentence pairs\n",
    "    \"\"\"\n",
    "    def __init__(self,datasettype):\n",
    "        #input data from English and Chinese token files\n",
    "        input_lang, output_lang, pairs = prepareData('en', 'zh',datasettype, True) \n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "        self.pairs = pairs\n",
    "        \n",
    "    def __len__(self):\n",
    "        #number of datapoints\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #get source and target sentence tensors by given index\n",
    "        src = tensorsFromPair(self.pairs[index])[0]\n",
    "        trg = tensorsFromPair(self.pairs[index])[1]\n",
    "        return src, trg\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Collate function for pytorch dataloader\"\"\"\n",
    "    \n",
    "    def _pad_sequences(seqs):\n",
    "        \"\"\"pad sentences with zeros to the maximum length\n",
    "        \n",
    "        Args:\n",
    "            seqs: given list of sentences\n",
    "        \n",
    "        Returns:\n",
    "            padded_seqs: list of padded sentences\n",
    "            lens: a list storing lengths of the sentences\n",
    "        \"\"\"\n",
    "        lens = [len(seq) for seq in seqs] #store lengths in a list\n",
    "        padded_seqs = torch.zeros(len(seqs), max(lens)).to(device) #pad by zero\n",
    "        for i, seq in enumerate(seqs):\n",
    "            end = lens[i]\n",
    "            padded_seqs[i, :end] = torch.cuda.LongTensor(seq[:end])\n",
    "        return padded_seqs, lens\n",
    "    \n",
    "    #sort according to length of src seqs\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True) \n",
    "    src_seqs, trg_seqs = zip(*data)\n",
    "    #pad sentences\n",
    "    src_seqs, src_lens = _pad_sequences(src_seqs)\n",
    "    trg_seqs, trg_lens = _pad_sequences(trg_seqs)\n",
    "\n",
    "    #(batch, seq_len) => (seq_len, batch)\n",
    "    src_seqs = src_seqs.transpose(0,1)\n",
    "    trg_seqs = trg_seqs.transpose(0,1)\n",
    "\n",
    "    return src_seqs, src_lens, trg_seqs, trg_lens\n",
    "\n",
    "#load and verify dataset \n",
    "input_lang, output_lang, pairs = prepareData('en', 'zh', 'train', True)\n",
    "val_dataset = Dataset('dev')\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=True)\n",
    "\n",
    "print(\"Number of source-target pairs:\", len(val_dataset))\n",
    "print(\"Input language: \"+ val_dataset.input_lang.name + '('+str(val_dataset.input_lang.n_words)+')')\n",
    "print(\"Output language: \"+ val_dataset.output_lang.name + '('+str(val_dataset.output_lang.n_words)+')')\n",
    "train_data = Dataset('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure models\n",
    "attn_model = 'dot'\n",
    "hidden_size = 256\n",
    "embed_size = 256\n",
    "n_layers = 1\n",
    "dropout = 0.1\n",
    "batch_size = 16\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "\n",
    "# Configure training/optimization\n",
    "clip = 50\n",
    "learning_rate = 0.001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "#         self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "#         if self.method == 'general':\n",
    "#             self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "#         elif self.method == 'concat':\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(1, hidden_size))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.normal_(mean=0, std=stdv)\n",
    "        \n",
    "    def forward(self, last_hidden, encoder_outputs, src_len=None):\n",
    "        \n",
    "        # Create variable to store attention energies\n",
    "        length = encoder_outputs.shape[0]\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        last_hidden = last_hidden.repeat(length,1,1)\n",
    "        energy =  torch.tanh(self.attn(torch.cat([last_hidden, encoder_outputs],dim=2)))\n",
    "        energy = energy.transpose(0, 1).transpose(1,2)\n",
    "        v = self.v.repeat(batch_size,1 , 1)\n",
    "        score = torch.bmm(v, energy)\n",
    "        if src_len is not None:\n",
    "            mask = []\n",
    "            for b in range(batch_size):\n",
    "                mask.append([0] * src_len[b] + [1] * (encoder_outputs.size(0) - src_len[b]))\n",
    "            mask = (torch.cuda.ByteTensor(mask).unsqueeze(1)) # [B,1,T]\n",
    "            score = score.masked_fill(mask, -1e9)\n",
    "        attn_weights = F.softmax(score, dim = 2)   \n",
    "        context_vector = torch.bmm(attn_weights, encoder_outputs.transpose(0,1))\n",
    "        # For each batch of encoder outputs\n",
    "        # Calculate energy for each encoder output\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "        # Return context vectors\n",
    "        return context_vector, attn_weights\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))              / math.sqrt(d_k)\n",
    "    mask = mask.transpose(0,3).repeat(1,scores.size(1),scores.size(2),1)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), h)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value =             [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous()              .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "    \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=200):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(0,1)\n",
    "        x = x + Variable(self.pe[:, :x.size(1)].repeat(x.size(0),1,1), \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class SelfAttnEncoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, N, h, d_model, d_ff, src_vocab, dropout):\n",
    "        super(SelfAttnEncoder, self).__init__()\n",
    "        attn = MultiHeadedAttention(h, d_model)\n",
    "        ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        layer = EncoderLayer(d_model, c(attn), c(ff), dropout)\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        position = PositionalEncoding(d_model, dropout)\n",
    "        self.src_embed= nn.Sequential(Embeddings(d_model, src_vocab), c(position))\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        x = self.src_embed(x.long())\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x).transpose(0,1)\n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features, device=device))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features, device=device))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "    \n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "def save_checkpoint(encoder, decoder, checkpoint_dir):\n",
    "    enc_filename = \"{}/enc-{}.pth\".format(checkpoint_dir, time.strftime(\"%d%m%y-%H%M%S\"))\n",
    "    dec_filename = \"{}/dec-{}.pth\".format(checkpoint_dir, time.strftime(\"%d%m%y-%H%M%S\"))\n",
    "    torch.save(encoder.state_dict(), enc_filename)\n",
    "    torch.save(decoder.state_dict(), dec_filename)\n",
    "    print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size, n_layer=1, dropout=0.1,attention=True):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "#         self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding_size = embedding_size\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size, padding_idx=PAD_TOKEN)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "#         self.gru = nn.GRU(hidden_size + embedding_size, hidden_size, n_layers, dropout=dropout, bidirectional=True)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, dropout=dropout, bidirectional=False)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "#         self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        if attention:\n",
    "            self.attn = Attn(hidden_size)\n",
    "    \n",
    "    def forward(self, input_seq, last_hidden=None, src_len = None, encoder_outputs=None):\n",
    "        batch_size = input_seq.size(0)\n",
    "        embedded = self.embedding(input_seq.long())\n",
    "        embedded = embedded.view(1, batch_size, self.embedding_size) # 1*B*E\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        if last_hidden:\n",
    "            last_hidden = (torch.sum(last_hidden[1], dim = 0).unsqueeze(0),\n",
    "                           torch.sum(last_hidden[1], dim = 0).unsqueeze(0))\n",
    "        rnn_output, hidden = self.lstm(embedded, last_hidden)\n",
    "#         rnn_output = rnn_output[:, :, :self.hidden_size] + rnn_output[:, :, self.hidden_size:] #1 * B * H\n",
    "        context, attn_weights = self.attn(rnn_output, encoder_outputs, src_len)\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = F.log_softmax(self.out(concat_output), dim=1)\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(src_batch, src_lens, trg_batch, trg_lens, encoder, decoder,\n",
    "               encoder_optimizer, decoder_optimizer, criterion):\n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss, em_accuracy, edit_distance = 0.0, 0.0, 0.0\n",
    "    # Run words through encoder\n",
    "    batch_size = src_batch.shape[1]\n",
    "    src_batch = src_batch.long()\n",
    "    src_mask  = (src_batch != PAD_TOKEN).unsqueeze(-2)\n",
    "    trg_mask = trg_batch.to(device)\n",
    "    trg_mask = (trg_mask != PAD_TOKEN).float()\n",
    "    encoder_outputs = encoder(src_batch, src_mask)\n",
    "    # Prepare input and output variables\n",
    "    decoder_input = torch.LongTensor([SOS_TOKEN] * batch_size).to(device)\n",
    "#     decoder_hidden = encoder_hidden[:decoder.n_layers*2] # Use last (forward) hidden state from encoder\n",
    "    max_trg_len = max(trg_lens)\n",
    "    decoder_hidden = None\n",
    "    # Run through decoder one time step at a time using TEACHER FORCING=1.0\n",
    "    TEACHER_FORCING = 1\n",
    "    for t in range(max_trg_len):\n",
    "        decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, \n",
    "                                                 decoder_hidden , src_lens, encoder_outputs)\n",
    "        if TEACHER_FORCING:\n",
    "            decoder_input = trg_batch[t]\n",
    "        else:\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi  # detach from history as input\n",
    "        loss += (criterion(decoder_output, trg_batch[t].long()) * trg_mask[t]).mean()\n",
    "    loss = loss / max_trg_len\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradient norms\n",
    "    enc_grads = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    dec_grads = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item(), em_accuracy, edit_distance #, enc_grads, dec_grads        \n",
    "\n",
    "def train(dataset, batch_size, n_epochs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, \n",
    "          checkpoint_dir=None, save_every=3000):\n",
    "    \n",
    "    writer=SummaryWriter('./tensorboard/self-attention-ZHEN')\n",
    "    global_step = 0 \n",
    "    \n",
    "\n",
    "    train_iter = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=True)\n",
    "    \n",
    "    fake_test = torch.utils.data.DataLoader(dataset=[train_data[i] for i in range(BATCH_SIZE * 10)],\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=False)\n",
    "    for i in range(n_epochs):\n",
    "        tick = time.process_time()\n",
    "        print(\"Epoch {}/{}\".format(i+1,  n_epochs))\n",
    "        losses, accs, eds = [], [], []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_iter):\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            input_batch, input_lengths, target_batch, target_lengths = batch\n",
    "            loss, accuracy, edit_distance = train_step(input_batch, input_lengths, target_batch, target_lengths,\n",
    "                                                       encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            \n",
    "            losses.append(loss)\n",
    "            accs.append(accuracy)\n",
    "            eds.append(edit_distance)\n",
    "            \n",
    "            writer.add_scalar('loss', loss, global_step)\n",
    "\n",
    "            if batch_idx % 100 == 0 and batch_idx != 0:\n",
    "                print(\"batch: {}, loss: {}, accuracy: {}, edit distance: {}\".format(batch_idx, loss, accuracy, \n",
    "                                                                                   edit_distance))\n",
    "            \n",
    "        tock = time.process_time()\n",
    "        print(\"Time: {} Avg loss: {} Avg acc: {} Edit Dist.: {}\".format(\n",
    "            tock-tick, np.mean(losses), np.mean(accs), np.mean(eds)))\n",
    "        save_checkpoint(encoder, decoder, checkpoint_dir)\n",
    "        bleu = evaluate(encoder, decoder, val_loader)\n",
    "        writer.add_scalar('val_bleu', bleu, global_step)\n",
    "        print('real:', bleu)\n",
    "\n",
    "        bleu_1 = evaluate(encoder, decoder, fake_test)\n",
    "        writer.add_scalar('train_bleu', bleu_1, global_step)\n",
    "        print('train_bleu:',bleu_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, test_loader, k=1, max_length=None):\n",
    "    output = []\n",
    "    h_t = []\n",
    "    p = []\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        src_batch, src_lens, trg_batch, trg_lens = batch\n",
    "        batch_size = src_batch.shape[1]\n",
    "#         pos_index = Variable(torch.LongTensor(range(batch_size)) * k).view(-1, 1)\n",
    "        with torch.no_grad():\n",
    "            decoded_sentences = []\n",
    "            for b in range(batch_size):\n",
    "                count += 1\n",
    "                max_length = src_lens[b]\n",
    "                trg_sentence = [output_lang.index2word[int(token)] for token in trg_batch[:,b] if token != PAD_TOKEN]\n",
    "                src_batch = src_batch.long()\n",
    "                src_input = src_batch[:src_lens[b],b].unsqueeze(1)\n",
    "                src_mask  = (torch.ones(src_lens[b],device = device)).unsqueeze(1).unsqueeze(-1)\n",
    "                encoder_outputs = encoder(src_input, src_mask)\n",
    "                decoder_input = torch.LongTensor([[SOS_TOKEN]]).to(device)\n",
    "                decoder_hidden = None\n",
    "                max_trg_len = trg_lens[b]\n",
    "                decoded_words = []\n",
    "                decoder_attentions = torch.zeros(batch_size, max_length, max_length)\n",
    "                priors = [[decoder_input, decoder_hidden, encoder_outputs,decoder_attentions,0, 0]]\n",
    "                sent_cand = ['' for i in range(k)]\n",
    "                for di in range(3 * max_length):\n",
    "                    curr = {}\n",
    "                    possible = []\n",
    "                    for prior_data in priors:\n",
    "                        decoder_input, decoder_hidden, encoder_outputs, decoder_attentions, v, source_idx = prior_data\n",
    "                        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                            decoder_input, decoder_hidden, [src_lens[b]], encoder_outputs)\n",
    "                        topv, topi = decoder_output.data.topk(k)\n",
    "#                         decoder_attentions[di] = decoder_attention.data\n",
    "                        for i in range(k):\n",
    "                            possible.append(int(topi[:,i].squeeze().detach()))\n",
    "                            curr[topv[0,i]+v] = [topi[:,i], decoder_hidden, encoder_outputs, \n",
    "                                                 decoder_attentions, topv[0,i]+v, source_idx]\n",
    "\n",
    "                    sorted_v = sorted(curr.keys(),reverse=True)\n",
    "                    top_k = sorted_v[:k]\n",
    "                    temp = [x for x in sent_cand]\n",
    "                    for i, index in enumerate(top_k):\n",
    "                        token = int(curr[index][0])\n",
    "                        source_idx = curr[index][-1]\n",
    "                        curr[index][-1] = i\n",
    "                        if token == EOS_TOKEN:\n",
    "                            sent_cand[i] = temp[source_idx] + '<eos>'\n",
    "                            break\n",
    "                        else:\n",
    "                            sent_cand[i] = temp[source_idx] + (output_lang.index2word[token] + \" \" )                    \n",
    "                    if EOS_TOKEN == possible[0]:\n",
    "                        decoded_words = sent_cand[0]\n",
    "                        break\n",
    "                    priors = [curr[index] for index in top_k]\n",
    "                if not decoded_words:\n",
    "                    decoded_words = '<eos>'\n",
    "                trg_sentence = ' '.join(trg_sentence)\n",
    "                s = corpus_bleu(decoded_words,trg_sentence).score\n",
    "                score += s\n",
    "        \n",
    "    return score/count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "c = copy.deepcopy\n",
    "h = 4\n",
    "d_model = 256\n",
    "d_ff = 512\n",
    "dropout = 0.1\n",
    "src_vocab = input_lang.n_words\n",
    "N = 4\n",
    "learning_rate = 0.0005\n",
    "\n",
    "\n",
    "encoder = SelfAttnEncoder(N, h, d_model, d_ff, src_vocab, dropout).to(device)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, output_lang.n_words, dropout=dropout).to(device)\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(),lr = learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(),lr = learning_rate)\n",
    "criterion = nn.NLLLoss(ignore_index = PAD_TOKEN)\n",
    "\n",
    "train(train_data, batch_size, n_epochs, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, criterion, checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
